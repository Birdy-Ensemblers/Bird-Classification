{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader, random_split, Subset\nimport os\nimport random\nimport cv2\nimport keras\nfrom tensorflow.keras.utils import load_img, img_to_array, array_to_img\nfrom keras.preprocessing.image import ImageDataGenerator","metadata":{"execution":{"iopub.status.busy":"2023-06-06T06:47:49.533897Z","iopub.execute_input":"2023-06-06T06:47:49.534381Z","iopub.status.idle":"2023-06-06T06:47:49.541549Z","shell.execute_reply.started":"2023-06-06T06:47:49.534343Z","shell.execute_reply":"2023-06-06T06:47:49.540143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check GPU Availability","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T06:20:42.975238Z","iopub.execute_input":"2023-06-06T06:20:42.976108Z","iopub.status.idle":"2023-06-06T06:20:43.010454Z","shell.execute_reply.started":"2023-06-06T06:20:42.976056Z","shell.execute_reply":"2023-06-06T06:20:43.009319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"## Visualize Sample Data","metadata":{}},{"cell_type":"code","source":"num_sample = 9\nroot = \"/kaggle/input/birds23sp/birds/train/\"\n\nclasses = []\nwith open(\"/kaggle/input/birds23sp/birds/names.txt\") as f:\n    classes.extend(f.readlines())\n    \nplt.figure(figsize = (20, 10))\nfor i in range(num_sample):\n    random_class = random.randint(0, 554)\n    class_name = classes[random_class].strip().split(\"\\n\")[0]\n    class_path = os.path.join(root, str(random_class))\n    files = os.listdir(class_path)\n    image = random.choice(files)\n    image_path = os.path.join(class_path, image)\n    image = load_img(image_path)\n    plt.subplot(3, 3, i + 1)\n    plt.imshow(image)\n    plt.title(class_name)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T06:23:05.493938Z","iopub.execute_input":"2023-06-06T06:23:05.494892Z","iopub.status.idle":"2023-06-06T06:23:08.593832Z","shell.execute_reply.started":"2023-06-06T06:23:05.494856Z","shell.execute_reply":"2023-06-06T06:23:08.591887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sample Size Distribution\nClass distribution is uneven. The class with the most instances is ten times the class with the least instances. The standard deviation is also very big.","metadata":{}},{"cell_type":"code","source":"path = \"/kaggle/input/birds23sp/birds/train/\"\n\nnum = 555\nclass_num = []\nfor i in range(num):\n    class_path = os.path.join(path, str(i))\n    files = os.listdir(class_path)\n    class_num.append(len(files))\n\nplt.figure(figsize = (5, 5))\nplt.xlabel(\"Images per class\")\nplt.ylabel(\"Num_classes\")\nplt.title(\"\")\nplt.hist(class_num)\n\nprint(f\"total images: {np.sum(class_num)}\")\nprint(f\"min class: {classes[np.argmin(class_num)].strip()}, num: {min(class_num)}\")\nprint(f\"max class: {classes[np.argmax(class_num)].strip()}, num: {max(class_num)}\")\nprint(f\"standard deviation: {np.std(class_num)}\")","metadata":{"execution":{"iopub.status.busy":"2023-06-06T06:23:55.797920Z","iopub.execute_input":"2023-06-06T06:23:55.798305Z","iopub.status.idle":"2023-06-06T06:24:04.931693Z","shell.execute_reply.started":"2023-06-06T06:23:55.798273Z","shell.execute_reply":"2023-06-06T06:24:04.930718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Image Size Statistics","metadata":{}},{"cell_type":"code","source":"height_list = []\nwidth_list = []\nfor i in range(num):\n    class_path = os.path.join(path, str(i))\n    files = os.listdir(class_path)\n    for file in files:\n        image_path = os.path.join(class_path, file)\n        height, width = cv2.imread(image_path).shape[:2]\n        height_list.append(height)\n        width_list.append(width)\n\nprint(f\"avg image height: {np.mean(height_list)}\")\nprint(f\"avg image width: {np.mean(width_list)}\")\nprint(f\"min image width: {min(width_list)}\")\nprint(f\"max image width: {max(width_list)}\")\nprint(f\"standard deviation: {np.std(width_list)}\")\n\nplt.figure(figsize = (5, 5))\nplt.xlabel(\"Image width\")\nplt.ylabel(\"Num_images\")\nplt.title(\"\")\nplt.hist(width_list)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T06:36:38.711684Z","iopub.execute_input":"2023-06-06T06:36:38.712045Z","iopub.status.idle":"2023-06-06T06:47:49.532124Z","shell.execute_reply.started":"2023-06-06T06:36:38.712015Z","shell.execute_reply":"2023-06-06T06:47:49.531197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Image Augmentation Visualization","metadata":{}},{"cell_type":"code","source":"img = load_img(\"/kaggle/input/birds23sp/birds/train/0/0b23d29cb6364a33a450f1f4fca010ac.jpg\")\n#plt.imshow(img)\nx = img_to_array(img)\nx = x.reshape((1,) + x.shape)\n\ndatagen = ImageDataGenerator(rotation_range=40, \n                             width_shift_range = 0.2,\n                             height_shift_range = 0.2,\n                             shear_range = 0.2,\n                             zoom_range = 0.2,\n                             fill_mode = \"nearest\",\n                             horizontal_flip=0.5)\n\nos.makedirs(\"/kaggle/working/augmented_data\")\n\ni=0\nfor img_batch in datagen.flow(x, batch_size=3, save_to_dir = \"/kaggle/working/augmented_data\"):\n    img_batch = img_batch.reshape(img_batch.shape[1:])\n    img_batch = array_to_img(img_batch)\n    image = img_batch\n    plt.subplot(3, 3, i + 1)\n    plt.imshow(image)\n    i += 1\n    if i >= 9:\n        break","metadata":{"execution":{"iopub.status.busy":"2023-06-06T06:50:37.285219Z","iopub.execute_input":"2023-06-06T06:50:37.285584Z","iopub.status.idle":"2023-06-06T06:50:41.753963Z","shell.execute_reply.started":"2023-06-06T06:50:37.285555Z","shell.execute_reply":"2023-06-06T06:50:41.753052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Weight Sampler","metadata":{}},{"cell_type":"code","source":"sample_weights = []\n\nfor i in range(num):\n    sample_weights.append(1 / class_num[i])\n    \nsampler = torch.utils.data.WeightedRandomSampler(weights=sample_weights, num_samples= 555 * 70)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T06:52:43.743973Z","iopub.execute_input":"2023-06-06T06:52:43.744423Z","iopub.status.idle":"2023-06-06T06:52:43.763011Z","shell.execute_reply.started":"2023-06-06T06:52:43.744391Z","shell.execute_reply":"2023-06-06T06:52:43.762009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"Here our data preprocessing pipeline includes image augmentation and train val splitting. ","metadata":{}},{"cell_type":"code","source":"def get_bird_data(batch_size=64, train_val_pct=0.2):\n    transform_data = transforms.Compose([\n        transforms.ToTensor(),\n    ])\n    \n    transform_train = transforms.Compose([\n        transforms.Resize(224),\n        transforms.RandomCrop(224, padding=8, padding_mode='edge'), # Take 128x128 crops from padded images\n        transforms.RandomHorizontalFlip(),    # 50% of time flip image along y-axis\n        transforms.RandomRotation(degrees=(0,180)),\n        transforms.RandomGrayscale(),\n        transforms.GaussianBlur(kernel_size=35),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n    \n    transform_test = transforms.Compose([\n        transforms.Resize(224),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n    \n    dataset = torchvision.datasets.ImageFolder(root='/kaggle/input/birds23sp/birds/train', transform=transform_data)\n    \n    num_data = len(dataset)\n    num_val = int(num_data * train_val_pct)\n    num_train = num_data - num_val\n    \n    trainset, valset = random_split(dataset, [num_train, num_val])\n    \n    trainset.dataset.transform = transform_train\n    valset.dataset.transform = transform_test\n    \n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True,)\n    valloader = torch.utils.data.DataLoader(valset, batch_size=1, shuffle=False, num_workers=2, pin_memory=True)\n    \n    testset = torchvision.datasets.ImageFolder(root='/kaggle/input/birds23sp/birds/test', transform=transform_test)\n    testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False, num_workers=2, pin_memory=True)\n    \n    classes = open(\"/kaggle/input/birds23sp/birds/names.txt\").read().strip().split(\"\\n\")\n    \n    # Backward mapping to original class ids (from folder names) and species name (from names.txt)\n    class_to_idx = dataset.class_to_idx\n    idx_to_class = {int(v): int(k) for k, v in class_to_idx.items()}\n    idx_to_name = {k: classes[v] for k,v in idx_to_class.items()}\n    return {'train': trainloader, 'val': valloader, 'test': testloader, 'to_class': idx_to_class, 'to_name':idx_to_name}\n\ndata = get_bird_data(batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T06:52:48.701154Z","iopub.execute_input":"2023-06-06T06:52:48.701820Z","iopub.status.idle":"2023-06-06T06:52:56.362276Z","shell.execute_reply.started":"2023-06-06T06:52:48.701787Z","shell.execute_reply":"2023-06-06T06:52:56.361330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Some Augmented Images","metadata":{}},{"cell_type":"code","source":"trainloader = data['train']\n\n# Get a batch of images from the trainloader\nimages, labels = next(iter(trainloader))\n\n# Visualize the images\nfig, axes = plt.subplots(figsize=(20, 10), ncols=5)\nfor i, ax in enumerate(axes):\n    ax.imshow(images[i].permute(1, 2, 0))  # Transpose image tensor from CxHxW to HxWxC\n    ax.axis('off')\n    ax.set_title(data['to_name'][labels[i].item()])\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-06T06:54:45.442637Z","iopub.execute_input":"2023-06-06T06:54:45.443430Z","iopub.status.idle":"2023-06-06T06:54:53.347104Z","shell.execute_reply.started":"2023-06-06T06:54:45.443385Z","shell.execute_reply":"2023-06-06T06:54:53.346147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"markdown","source":"The function doesn't include the weight sampler because we found that it even decrease the performance of our model predictions on the leaderboard. But theoretically, it should work better.","metadata":{}},{"cell_type":"code","source":"def train(net, trainloader, valloader, epochs=10, start_epoch=0, lr=0.01, momentum=0.9, decay=0.0005, \n          verbose=1, print_every=10, state=None, schedule={}, checkpoint_path=None):\n    net.to(device)\n    net.train()\n    losses = []\n    losses_val = []\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum, weight_decay=decay)\n#     optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=decay)\n\n    # Load previous training state\n    if state:\n        net.load_state_dict(state['net'])\n        optimizer.load_state_dict(state['optimizer'])\n        start_epoch = state['epoch']\n        losses = state['losses']\n        losses_val = state['losses_val']\n\n    # Fast forward lr schedule through already trained epochs\n    for epoch in range(start_epoch):\n        if epoch in schedule:\n            print (\"Learning rate: %f\"% schedule[epoch])\n            for g in optimizer.param_groups:\n                g['lr'] = schedule[epoch]\n\n    for epoch in range(start_epoch, epochs):\n        sum_loss = 0.0\n        running_loss = 0.0\n        sum_loss_val = 0.0\n\n        # Update learning rate when scheduled\n        if epoch in schedule:\n            print (\"Learning rate: %f\"% schedule[epoch])\n            for g in optimizer.param_groups:\n                g['lr'] = schedule[epoch]\n                \n        num_train = 0\n        for i, batch in enumerate(trainloader, 0):\n            \n            inputs, labels = batch[0].to(device), batch[1].to(device)\n\n            optimizer.zero_grad()\n\n            outputs = net(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()  # autograd magic, computes all the partial derivatives\n            optimizer.step() # takes a step in gradient direction\n\n            losses.append(loss.item())\n            sum_loss += loss.item()\n            running_loss += loss.item()\n            \n            num_train += 1\n\n            if i % print_every == print_every-1:    # print every 100 mini-batches\n                if verbose:\n                    print('[epoch %d, batch %d] train loss: %.3f' % (epoch, i + 1, running_loss / print_every))\n                    running_loss = 0.0\n            \n        net.eval()\n        \n        with torch.no_grad():\n            num_val = 0\n            for inputs, labels in valloader:\n                inputs = inputs.to(device)  # Move inputs to the device\n                labels = labels.to(device)\n                outputs = net(inputs)  # Forward pass\n                loss = criterion(outputs, labels)\n                sum_loss_val += loss.item()\n                num_val += 1\n                \n            losses_val.append(sum_loss_val / num_val)\n            \n        if verbose:\n            print('[epoch %d] train loss: %.3f, val loss: %.3f' % (epoch, sum_loss / num_train, sum_loss_val / num_val))\n            \n        if checkpoint_path:\n            state = {'epoch': epoch+1, 'net': net.state_dict(), 'optimizer': optimizer.state_dict(), 'losses': losses}\n            torch.save(state, checkpoint_path + 'checkpoint-%d.pth'%(epoch+1))\n    return losses, losses_val","metadata":{"execution":{"iopub.status.busy":"2023-06-06T06:57:47.753671Z","iopub.execute_input":"2023-06-06T06:57:47.754331Z","iopub.status.idle":"2023-06-06T06:57:47.776615Z","shell.execute_reply.started":"2023-06-06T06:57:47.754282Z","shell.execute_reply":"2023-06-06T06:57:47.775549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Pretrained Models","metadata":{}},{"cell_type":"markdown","source":"Download [MaxViT: Multi-Axis Vision Transformer](http://arxiv.org/abs/2204.01697) (tiny version). Change the output size of the last layer to 555.","metadata":{}},{"cell_type":"code","source":"maxvit = torchvision.models.maxvit_t(weights='DEFAULT')\nmaxvit.classifier[5] = nn.Linear(512, 555)\nprint(maxvit)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T06:59:51.040063Z","iopub.execute_input":"2023-06-06T06:59:51.040785Z","iopub.status.idle":"2023-06-06T06:59:53.332807Z","shell.execute_reply.started":"2023-06-06T06:59:51.040751Z","shell.execute_reply":"2023-06-06T06:59:53.331509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Download [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545) (small version). Change the output size of the last layer to 555.","metadata":{}},{"cell_type":"code","source":"convnext = torchvision.models.convnext_small(weights='DEFAULT')\nconvnext.classifier[2] = nn.Linear(768, 555)\nprint(convnext)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T07:04:09.036555Z","iopub.execute_input":"2023-06-06T07:04:09.036950Z","iopub.status.idle":"2023-06-06T07:04:12.751706Z","shell.execute_reply.started":"2023-06-06T07:04:09.036919Z","shell.execute_reply":"2023-06-06T07:04:12.749972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Download [Swin Transformer V2: Scaling Up Capacity and Resolution](https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Swin_Transformer_V2_Scaling_Up_Capacity_and_Resolution_CVPR_2022_paper.pdf) (tiny version). Change the output size of the last layer to 555.","metadata":{}},{"cell_type":"code","source":"swin = torchvision.models.swin_v2_t(weights='DEFAULT')\nswin.head = nn.Linear(768, 555)\nprint(swin)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T07:04:12.753765Z","iopub.execute_input":"2023-06-06T07:04:12.754135Z","iopub.status.idle":"2023-06-06T07:04:14.468980Z","shell.execute_reply.started":"2023-06-06T07:04:12.754096Z","shell.execute_reply":"2023-06-06T07:04:14.468138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Start Training or Load Weight Files","metadata":{}},{"cell_type":"code","source":"# losses, losses_val = train(maxvit, data['train'], data['val'], epochs=10, checkpoint_path='/kaggle/working/', print_every=100, lr=0.005)\ncp_max = torch.load('/kaggle/input/birds-classification-models/maxvit-t-checkpoint-8.pth')\nmaxvit.load_state_dict(cp_max['net'])","metadata":{"execution":{"iopub.status.busy":"2023-06-06T07:06:53.244560Z","iopub.execute_input":"2023-06-06T07:06:53.244943Z","iopub.status.idle":"2023-06-06T07:06:59.162266Z","shell.execute_reply.started":"2023-06-06T07:06:53.244914Z","shell.execute_reply":"2023-06-06T07:06:59.161365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# losses, losses_val = train(convnext, data['train'], data['val'], epochs=15, checkpoint_path='/kaggle/working/', print_every=100, lr=0.005)\ncp_conv = torch.load('/kaggle/input/birds-classification-models/convnext-small-checkpoint-8.pth')\nconvnext.load_state_dict(cp_conv['net'])","metadata":{"execution":{"iopub.status.busy":"2023-06-06T07:06:59.164104Z","iopub.execute_input":"2023-06-06T07:06:59.164630Z","iopub.status.idle":"2023-06-06T07:07:07.548681Z","shell.execute_reply.started":"2023-06-06T07:06:59.164596Z","shell.execute_reply":"2023-06-06T07:07:07.547725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# losses, losses_val = train(swin, data['train'], data['val'], epochs=15, checkpoint_path='/kaggle/working/', print_every=100, lr=0.005)\ncp_swin = torch.load('/kaggle/input/birds-classification-models/swin-v2-t-checkpoint-7.pth')\nswin.load_state_dict(cp_swin['net'])","metadata":{"execution":{"iopub.status.busy":"2023-06-06T07:07:07.550057Z","iopub.execute_input":"2023-06-06T07:07:07.550491Z","iopub.status.idle":"2023-06-06T07:07:10.601014Z","shell.execute_reply.started":"2023-06-06T07:07:07.550456Z","shell.execute_reply":"2023-06-06T07:07:10.599999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def smooth(x, size):\n    return np.convolve(x, np.ones(size)/size, mode='valid')","metadata":{"execution":{"iopub.status.busy":"2023-06-06T07:13:13.410053Z","iopub.execute_input":"2023-06-06T07:13:13.411005Z","iopub.status.idle":"2023-06-06T07:13:13.416047Z","shell.execute_reply.started":"2023-06-06T07:13:13.410960Z","shell.execute_reply":"2023-06-06T07:13:13.415093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble","metadata":{}},{"cell_type":"markdown","source":"## Compute the Best Coefficient for Ensemble of Two Models","metadata":{}},{"cell_type":"code","source":"def compute_weight_2(net1, net2):\n    net1.to(device)\n    net1.eval()\n    net2.to(device)\n    net2.eval()\n    accuracies = []\n    alphas = []\n    \n    for alpha in range(5, 96, 5):\n        print(alpha)\n        num_correct = 0\n        total = 0\n        for i, batch in enumerate(data['val'], 0):\n            if i%100==0: print(i, end = ' ')\n            \n            inputs, labels = batch[0].to(device), batch[1].to(device)\n            outputs1 = net1(inputs)\n            outputs2 = net2(inputs)\n            outputs = outputs1*alpha/100 + outputs2*(1-alpha/100)\n            _, pred = torch.max(outputs.data, 1)\n            for p, l in zip(pred, labels):\n                total += 1\n                if p == l: num_correct += 1\n            if i==999: break\n        print()\n        accuracies.append(num_correct / total)\n        alphas.append(alpha/100)\n        \n    weights = {}\n    \n    for i in range(len(accuracies)):\n        weights[accuracies[i]] = alphas[i]\n        \n    sortedWeights = dict(sorted(weights.items()))\n    \n    print(sortedWeights)\n    \n    return sortedWeights","metadata":{"execution":{"iopub.status.busy":"2023-06-06T07:13:15.265107Z","iopub.execute_input":"2023-06-06T07:13:15.265487Z","iopub.status.idle":"2023-06-06T07:13:15.276121Z","shell.execute_reply.started":"2023-06-06T07:13:15.265457Z","shell.execute_reply":"2023-06-06T07:13:15.275107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The result shows that the best coefficient between MaxViT and ConvNeXt is 0.6 : 0.4.","metadata":{}},{"cell_type":"code","source":"# weights = compute_weight_2(maxvit, convnext)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T07:13:15.917965Z","iopub.execute_input":"2023-06-06T07:13:15.918546Z","iopub.status.idle":"2023-06-06T07:13:15.923368Z","shell.execute_reply.started":"2023-06-06T07:13:15.918514Z","shell.execute_reply":"2023-06-06T07:13:15.922147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Compute the Best Coefficient for Ensemble of Three Models","metadata":{}},{"cell_type":"code","source":"def compute_weight_3(net1, net2, net3, weight1):\n    net1.to(device)\n    net1.eval()\n    net2.to(device)\n    net2.eval()\n    net3.to(device)\n    net3.eval()\n    accuracies = []\n    alphas = []\n    \n    for alpha in range(5, 96, 5):\n        print(alpha)\n        num_correct = 0\n        total = 0\n        for i, batch in enumerate(data['val'], 0):\n            if i%100==0: print(i, end = ' ')\n            \n            inputs, labels = batch[0].to(device), batch[1].to(device)\n            outputs1 = net1(inputs)\n            outputs2 = net2(inputs)\n            outputs12 = outputs1 * weight1 + outputs2 * (1 - weight1)\n            outputs3 = net3(inputs)\n            outputs = outputs12*alpha/100 + outputs3*(1-alpha/100)\n            _, pred = torch.max(outputs.data, 1)\n            for p, l in zip(pred, labels):\n                total += 1\n                if p == l: num_correct += 1\n            if i==999: break\n        print()\n        accuracies.append(num_correct / total)\n        alphas.append(alpha/100)\n        \n    weights = {}\n    \n    for i in range(len(accuracies)):\n        weights[accuracies[i]] = alphas[i]\n        \n    sortedWeights = dict(sorted(weights.items()))\n    \n    print(sortedWeights)\n    \n    return sortedWeights","metadata":{"execution":{"iopub.status.busy":"2023-06-06T07:13:17.149713Z","iopub.execute_input":"2023-06-06T07:13:17.150069Z","iopub.status.idle":"2023-06-06T07:13:17.160622Z","shell.execute_reply.started":"2023-06-06T07:13:17.150040Z","shell.execute_reply":"2023-06-06T07:13:17.159488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The result shows that the best coefficient between MaxViT, ConvNeXt, and Swin Transformer V2 is 0.39 : 0.26 : 0.35.","metadata":{}},{"cell_type":"code","source":"# weights = compute_weight_3(maxvit, convnext, swin, 0.6)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T07:13:17.976412Z","iopub.execute_input":"2023-06-06T07:13:17.976762Z","iopub.status.idle":"2023-06-06T07:13:17.983575Z","shell.execute_reply.started":"2023-06-06T07:13:17.976733Z","shell.execute_reply":"2023-06-06T07:13:17.982658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"markdown","source":"Prediction using one model. ","metadata":{}},{"cell_type":"code","source":"def predict(net, dataloader, ofname):\n    out = open(ofname, 'w')\n    out.write(\"path,class\\n\")\n    net.to(device)\n    net.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for i, (images, labels) in enumerate(dataloader, 0):\n            if i%100 == 0:\n                print(i)\n            images, labels = images.to(device), labels.to(device)\n            outputs = net(images)\n            _, predicted = torch.max(outputs.data, 1)\n            fname, _ = dataloader.dataset.samples[i]\n            out.write(\"test/{},{}\\n\".format(fname.split('/')[-1], data['to_class'][predicted.item()]))\n    out.close()","metadata":{"execution":{"iopub.status.busy":"2023-06-06T07:13:19.601104Z","iopub.execute_input":"2023-06-06T07:13:19.601769Z","iopub.status.idle":"2023-06-06T07:13:19.612450Z","shell.execute_reply.started":"2023-06-06T07:13:19.601736Z","shell.execute_reply":"2023-06-06T07:13:19.611488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Prediction using two models. ","metadata":{}},{"cell_type":"code","source":"def predict_ensemble_2(net1, net2, weight, dataloader, ofname):\n    out = open(ofname, 'w')\n    out.write(\"path,class\\n\")\n    net1.to(device)\n    net1.eval()\n    net2.to(device)\n    net2.eval()\n    with torch.no_grad():\n        for i, (images, labels) in enumerate(dataloader, 0):\n            if i%100 == 0:\n                print(i)\n            images, labels = images.to(device), labels.to(device)\n            outputs1 = net1(images)\n            outputs2 = net2(images)\n            outputs = outputs1*weight + outputs2*(1-weight)\n            _, predicted = torch.max(outputs.data, 1)\n            fname, _ = dataloader.dataset.samples[i]\n            out.write(\"test/{},{}\\n\".format(fname.split('/')[-1], data['to_class'][predicted.item()]))\n    out.close()","metadata":{"execution":{"iopub.status.busy":"2023-06-06T07:13:20.288923Z","iopub.execute_input":"2023-06-06T07:13:20.289821Z","iopub.status.idle":"2023-06-06T07:13:20.299426Z","shell.execute_reply.started":"2023-06-06T07:13:20.289773Z","shell.execute_reply":"2023-06-06T07:13:20.298049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Prediction using three models. ","metadata":{}},{"cell_type":"code","source":"def predict_ensemble_3(net1, net2, net3, weight1, weight2, dataloader, ofname):\n    out = open(ofname, 'w')\n    out.write(\"path,class\\n\")\n    net1.to(device)\n    net1.eval()\n    net2.to(device)\n    net2.eval()\n    net3.to(device)\n    net3.eval()\n    with torch.no_grad():\n        for i, (images, labels) in enumerate(dataloader, 0):\n            if i%100 == 0:\n                print(i)\n            images, labels = images.to(device), labels.to(device)\n            outputs1 = net1(images)\n            outputs2 = net2(images)\n            outputs12 = outputs1*weight1 + outputs2*(1-weight1)\n            outputs3 = net3(images)\n            outputs = outputs12*weight2 + outputs3*(1-weight2)\n            _, predicted = torch.max(outputs.data, 1)\n            fname, _ = dataloader.dataset.samples[i]\n            out.write(\"test/{},{}\\n\".format(fname.split('/')[-1], data['to_class'][predicted.item()]))\n    out.close()","metadata":{"execution":{"iopub.status.busy":"2023-06-06T07:13:20.971686Z","iopub.execute_input":"2023-06-06T07:13:20.972318Z","iopub.status.idle":"2023-06-06T07:13:20.981681Z","shell.execute_reply.started":"2023-06-06T07:13:20.972277Z","shell.execute_reply":"2023-06-06T07:13:20.980349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Final submission. This version gets 1st place (1/32) in both the public and private leaderboard during 2023 Spring, with the accuracy of 88% and 88.212%.","metadata":{}},{"cell_type":"code","source":"predict_ensemble_3(maxvit, convnext, swin, 0.6, 0.65, data['test'], \"submissions_ensemble_maxvit_t_convnext_s_swin_v2_t.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-06-03T07:37:22.368795Z","iopub.execute_input":"2023-06-03T07:37:22.369177Z","iopub.status.idle":"2023-06-03T07:47:26.310649Z","shell.execute_reply.started":"2023-06-03T07:37:22.369147Z","shell.execute_reply":"2023-06-03T07:47:26.309321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Some Code References\n[Data Preprocessing Template](https://www.kaggle.com/code/yuriihalychanskyi/cse-455-final-birds)\n\n[Training and Prediction Template](https://www.kaggle.com/code/pjreddie/transfer-learning-to-birds-with-resnet18)\n\n[Ensemble Template](https://github.com/parkr98/Bird_Identification)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}